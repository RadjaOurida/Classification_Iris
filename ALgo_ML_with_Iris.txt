# Prendre le dataset iris.csv

#Help sur des librairies ou des méthodes : help(nomMethode)
 help(rpart) 
 help(rpart.control)

#Importer un dataset dans R
  data(iris)

#Afficher tous le dataset 
  iris

#Afficher klk lignes du dataset avec toutes les colonnes : iris[1:numero de la dernière ligne,]
  iris[1:10,]

#Afficher klk colonnes seulement   
  iris[,2:5]

#Afficher klk lignes du dataset avec klk  colonnes : iris[numero de la 1er ligne:numero de la dernière ligne,numero de la 1er colonne:numero de la dernière colonne]
   iris[2:25, 1:4]

#Afficher les n=3 première ligne du sataset avec tt ses colones
 head(iris,3)

#Selectionner des attribus iet on les met dans une variable "dataIris" ; 
#dataIris contient tt les lignes et klk colonnes
 dataIris =iris[,2:5]

# Nature des attribus ie afficher le type des colonnes de cet ensemble de donnée
 lapply (iris,class)
 lapply(dataIris,class)

# Dimension d'un dataset (nbr ligne et nbr colonnes)
 dim(iris)
 nbrLigne=dim(iris)[1]
 nbrAttribut=dim(iris)[2]

#Graphe d'un dataset, d'une partie du dataset ou de deux attributs, on affiche chacun des attribut par rapport à 
#la sortie
 plot(iris)
 plot(dataIris)
 plot(iris$Sepal.Length,iris$Sepal.Width)

#enlever une colone du dataset, enlever le 2em attribut
irisMoins=iris[,-2] 
irisMoins =iris[1:150,-2] #c'est l'ekivalent du 1er


#  Apprentissage supervisé : Arbre de décision, Foret Aléatoire......


########################################################### Arbre de décision 'Decision tree'#######################################

# utiliser rpart et rpart.plot, on doit donc importer ces deux packages 
 library(rpart)
 library(rpart.plot)

#Pour afficher les détails sur rpart et rpart.control
 help(rpart) 

#Importer les données ou le dataset sur leuel nous allons travailler
 data(iris)

#===============================Créer l'arbre de décision simple
 irisTree=rpart(Species~., data=iris)

#Afficher l'arbre de décision crée 
 plot(irisTree)
 text(irisTree) #pour étiqité les noeuds

#La prédiction
predict(irisTree, iris[1:150,],type="class")

#La table de confusion, pour voir si l'arbre est efficace ou non
 table(iris$Species,predict(irisree,iris[1:150,],type="class"))

#==============================Créer l'arbre de décision avec selection d'attribut

#Faire une selection d'attribut, les attribut qui ont contribué dans l'arbre précedant pour arriver directement au résultat voulu

 irisSelect=iris[,3:5]
 irisTree=rpart(Species~., data=irisSelect)
 plot(irisTree)
 text(irisTree) #pour étikité les noeuds
 prediction =predict(irisTree, irisSelect[1:150,],type="class")
 table(irisSelect$Species,prediction)

#Nous retrouvons le meme arbre et la meme table de décision car l'arbre de décision se base sur 
#la selection d'attribut d'ailleurs on remarque que dans le code précedant il a choisit les attribut
#les plus pértinent.



##===============================créer un arbre avec klk controle (ajouter des contraintes pour obtenir un arbre elagué ie plus précis)
#cp c'est le taux d'erreur minimum dans chaque noeud si taux d'erreur pour l'une des var > cp continuer à elaguer l'arbre 
# nsplit: le nombre minimum d'observations(ligne) dans un noeud si nbrLigne=nsplit et tauxErreur>cp ne pas élaguer l'arbre, si nbrLigne=nsplit et tauxErreur>cp elaguer
#Pour avoir un arbre qui donne une précision à 100% on met minsplit=0, cp=0) mais si le dataset est grand l'arbre sera très grand
#In english it is called pruning

  irisTreeElague <- rpart(Species ~., data=iris, control=rpart.control(minsplit=5, cp =0.01)) 
  
   #irisTreeElague <- rpart(Species ~., data=iris, control=rpart.control(minsplit=0, cp =0)) 

#Afficher cet arbre avec des information sur les noeuds
 plot(irisTreeElague)
 text(irisTreeElague)
 prp(irisTreeElague) #afficher des noeuds ronds

#Afficher pour chaque noeud étape par étape sa construction (les taux d'erreurs ...etc)
 print(irisTreeElague)

#Predire des éléments du dataset ou tout le dataset iris par cet arbre la,il s'agit de jeter les éléments dans l'arbre puis comparer l'élément de la feuille avec la sortie réelle 
 predict(irisTreeElague, iris[1:150,],type="class")

#La table de confusion, pour voir si l'arbre est efficace ou non
 table(iris$Species,predict(irisreeElargue,iris[1:150,],type="class"))

# Le résultat change en changeant les parametres du controle cp et minsplit et ça donne de meilleurs résultats par rapport aux premières méthodes.


##################################################### Foret aléatoires 'Random forest'################################################

labrary(randomForest)
data(iris)
#=============================RandomForest Simple
RF= randomForest(Species~., iris)
RF
prediction= predict(RF,iris[1:150,-5],type="class")
combintables = cbind(iris, prediction)
table(combintables $Species,combintables$prediction)


#=============================Random forest avec envlopante (cross validation CV)

#Déviser le dataset en 0.7 et 0.3
 n=dim(iris)[1] 
 index=sample(n , n*0.7)

#création des deux parties (apprentissage et test)
 apprentissage = iris[index,]
 #x= apprentissage[,-5] y=  apprentissage$Outcome
 test = iris[-index, -5]
#ou bien test2=iris[-index,] le resulat sera le meme

#création du modèle
 #RandomF.model= train(x,y,'rf',trControl=trainControl(method='cv',number=10))
 RandomF.model=randomForest(Species~., data= apprentissage)

#Prédir le test (ou test2 kifkif) par ce modèle le résultat est pls ligne mais une seule colonne (species predites)
prediction = predict(object=RandomF.model,newdata= test)


#Comparaison des sorties prédites et les vrai valeurs du test
# pour cela combiner le tableau prediction avec test (ça c'est obligatoire)
#si on combine avec test2 les résultat ne sera pas interessant car par la suite on souhaite faire une comparaison d'ou test 
 testPrediction= cbind(test, prediction )
 testPrediction

#table de confusion entre les deux dernière colonnes du tableau testpreiction ie Species et predict
 table(testPrediction$Species, testPrediction$prediction)

 
############################################################ Naive Bayes '#########################################################


labrary(e1071)
data(iris)
#=============================Naive Bayes Simple 

#creer le model naive bayes
 modelNB= naiveBayes(Species~., data=iris)
 NB
#Prédire la sortie des données 
 newdata= iris[100:150, -5]
 prediction= predict(modelNB, newdata)

#combiner les classe prédite avec la partie du dataset pour comparer les sortie prédites et les vraies classes 
 newDataPrediction = cbind(iris[100:150,], prediction)
 newDataPrediction

#Table de confusion
table(newDataPrediction$Species, newDataPrediction$prediction)

#Afficher en couleur les classes
 plot(cmdscale(dist(iris[,-5])),col = as.integer(iris[,5]),pch = c("o","+")[1:150 %in% modelNB$index + 1])
 pch = c("o","+")[1:150 %in% modelNB$index + 1]


#=============================Naive Bayes cross validation

#Déviser le dataset en 0.7 et 0.3
 n=dim(iris)[1] 
 index=sample(n , n*0.7)

#création des deux parties (apprentissage et test)
 apprentissage = iris[index,]
 x= apprentissage[,-5] 
 y= apprentissage$Outcome
 test = iris[-index,]
#ou bien test2=iris[-index,-5] le resulat sera le meme

#création du modèle
 # model= train(x,y,'nb',trControl=trainControl(method='cv',number=10)) on doit avoir un package kui contient train
 model= naiveBayes(Species~., data=apprentissage)

#prédir le test 
 Predict <- predict(model,newdata = test)

#table de confusion
 table(Predict, test[,5] )

################################################Support Vector Machine  ' SVM '######################################################################################

data(iris)
attach(iris)
 model <- svm(Species ~ ., data = iris)
newdata=iris[1:2,-5]
 pred <- predict(model, newdata)
 pred1 <- predict(model, newdata, decision.values = TRUE)
table1 =table(pred, iris[1:2,5])
# visualize (classes by color, SV by crosses):
 plot(cmdscale(dist(iris[,-5])),col = as.integer(iris[,5]),pch = c("o","+")[1:150 %in% model$index + 1])
   pch = c("o","+")[1:150 %in% model$index + 1])





############################################# Réseau de Nerone ###############################################################################################

 library(nnet)
 data(iris)
 n=dim(iris)[1]
 index = sample(n, 0.7 * n)
 Appren = iris[index, ]
 Test = iris[-index, ]
 RN <- nnet(Species~., Appren, size = 2, rang = 0.1,  decay = 5e-4, maxit = 200)
 summary(RN)
 pred=predict(RN, newdata=Test)
 Test.mod <- cbind(Test, Pred)
 
 table( Test.mod$Species, Test.mod$Pred)

###################################################################################################################################





library(dbscan)
(kohonen)
library(arules)
